%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{soul}

\usepackage{makecell}
\usepackage{circuitikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

\renewcommand{\vec}[1]{\mathbf{#1}} % re-style the vector
\usepackage{tikz}
\usetikzlibrary{
  arrows.meta, % for Straight Barb arrow tip
  fit, % to fit the group box around the central neurons
  positioning, % for relative positioning of the neurons
}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

\def\layersep{2.5cm}

\tikzset{
  neuron/.style={ % style for each neuron
    circle,draw,thick, % drawn as a thick circle
    inner sep=0pt, % no built-in padding between the text and the circle shape
    minimum size=3.5em, % make each neuron the same size regardless of the text inside
    node distance=2em and 4em, % spacing between neurons (y and x)
  },
  group/.style={ % style for the groups of neurons
    rectangle,draw,thick, % drawn as a thick rectangle
    inner sep=0pt, % no padding between the node contents and the rectangle shape
  },
  output/.style={ % style for the inputs/outputs
    neuron, % inherit the neuron style
    fill=gray!15, % add a fill color
  },
  input/.style={ % style for each neuron
    circle%,draw,thick, % drawn as a thick circle
    inner sep=0pt, % no built-in padding between the text and the circle shape
    minimum size=2.5em, % make each neuron the same size regardless of the text inside
    node distance=2em and 4em, % spacing between neurons (y and x)
  },
  conn/.style={ % style for the connections
    -{Straight Barb[angle=60:2pt 3]}, % simple barbed arrow tip
    thick, % draw in a thick weight to match other drawing elements
  },
}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

% XXX Potentially get rid of "Deep"
\icmltitlerunning{Can Inter-layer Recurrent Neural Networks Learn Algorithms?}

\begin{document}

\twocolumn[
% XXX Potentially get rid of "Deep"
% XXX Potentially talk about algorithm learning, program synthesis
\icmltitle{Can Inter-layer Recurrent Neural Networks Learn Algorithms?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anthony Proschka}{can}
\end{icmlauthorlist}

\icmlaffiliation{can}{Machine Learning Group, Candis GmbH, Berlin, Germany}

\icmlcorrespondingauthor{Anthony Proschka}{anthony@candis.io}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
While recurrent neural networks have been proven to express universal Turing machines, \textit{training} them to perform arbitrary computable functions exactly is difficult. In this paper, we introduce a novel architecture called Inter-layer Recurrent Neural Network (ILRNN), a form of deep recurrent neural network that exhibits extended and top-down connectivity across layers. Inspired by digital circuit design, we show that an ILRNN with informed initial hyperparameter configuration \hl{can learn} to add arbitrarily large binary numbers. It remains to be evaluated whether a learning algorithm exists that can reliably find global minima in such a network from all possible initializations.

% \begin{itemize}
%     \item and show that there exists a configuration of parameters and hyperparameters that solves the given task exactly. However, we only succeed in training the architecture to solve the task exactly in a small share of trials, despite testing various settings for the activation function and its gradient, the training objective (and regularization), the optimization algorithm and its hyperparameters (such as the learning rate)
%     % pronounced /aɪlɜːn/)
% \end{itemize}
\end{abstract}

\section{Introduction \& Motivation}
\label{motivation}

In recent years, deep learning has shown remarkable progress in applications such as image recognition (\hl{XXX}), speech recognition (\hl{XXX}), games (\hl{XXX}) and photorealistic image generation (\hl{XXX}), either being the first technology to ever solve a problem or achieve human parity on it.

Despite these successes, deep neural networks and the current means to train them still face an array of difficulties: sample complexity, overfitting, and the convergence to potentially useful yet imperfect solutions. \hl{XXX}.

In this paper, we want to draw the attention to three core aspects of the study of artificial neural networks: expressivity, learnability and interpretability.

An important distinction that needs to be made is the fundamental difference between the expressivity of a deep learning model and its learnability. In the realm of real functions, several theoretical works have shown that feed forward artificial neural networks act as universal function approximators, i.e. under certain conditions they are able to approximate up to some tolerated error any arbitrary function (\hl{XXX}). In the realm of theory of computation, Siegelman and Sontag have shown that recurrent neural networks are able to simulate any Turing machine, which is equivalent to saying they can compute any partially recursive function (\hl{???}). Universal function approximation and Turing completeness are structurally different properties and should not be equated. However, they both illustrate that given the right set of hyperparameters and a viable weight configuration, artificial neural networks should in theory be able express a solution to any given task.

While expressivity seems to be a well established property of ANNs, their learnability is much less so. An early work by \hl{XXX}  showed that under certain circumstances, even the smallest networks are NP-hard to train (\hl{XXX}). In most scenarios, the system of polynomial or nonlinear equations resulting from modelling a given set of labelled data using a neural network cannot easily be solved analytically. Other solution and optimization techniques seem to have little guarantees that they find any solution or a global optimum (\hl{???}).

Finally, ANNs are still mostly considered "black box" functions that lack interpretability (\hl{XXX}). Once trained to perform a specific task, it is not straightforward to reconstruct the 

\begin{itemize}
    \item Biological plausibility: not really, since it's unlikely that biological neurons really have logic gate behavior, right?
    \item But: my approach suggests that - in artificial neural networks -
    \item We of course somewhat get rid of this whole hierarchical representations topic
\end{itemize}

Our contributions include the following:

\begin{itemize}
    \item We introduce a novel architecture for deep recurrent neural networks   called inter-layer recurrent neural networks (ILRNN). These networks exhibit extended and top-down connectivity across time steps without introducing cyclical paths in the computational graph. 
    \item Building on top of well-known insights that neural networks can compute logic gates, we show by an example that recurrent neural networks can in fact represent any (\hl{XXX}) Boolean circuit perfectly. Thusly, ANNs with the appropriate activation functions can be interpreted as trainable versions of combinational logic.
    \item Formulating a (almost) memoryless binary addition task, we \hl{XXX}
    \item Using various of today's solving and optimization techniques, we obtain the empirical result that finding the weight configuration that solves the task perfectly is hard.
\end{itemize}

\section{Related Work}
\label{related-work}

In the following section we quickly recap important milestones in the advancements of recurrent neural networks (RNNs) and in the attempts to teach neural networks to learn exact and general solutions to algorithmic and computational problems.

\subsection{Recurrent neural networks}

Rumelhart et al. were possibly the first researchers to study recurrent neural networks (RNNs) as an extension of standard feedforward neural networks with the intention to model sequence and especially time series data (\citeyear{rumelhart1986learning}). They show how their then-novel backpropagation algorithm could be applied to RNNs by unrolling the net for a finite number of time steps and backpropagating error gradients through these (BPTT, \hl{XXX}).

Around the same time, Jordan publishes a report on using RNNs for sequence modeling (\hl{Serial Order}). There, he also introduces simple nets that have recurrent connections feeding the outputs of a previous time step back to the hidden nodes of the next (note this is a first record of the idea generalized by ILRNNs that will be introduced later in this paper).

One of the major advances in RNN research is the long short-term memory cell (\hl{Schmidhuber, Hochreiter}). By incorporating gating mechanisms to an RNN cell that control which \hl{XXX}, they circumvent the so-called vanishing or exploding gradient problem that arises when gradients are backpropagated through many layers of neurons in conventional networks (\hl{XXX}). LSTMs and similar models such as GRUs are today widely used in practice (\hl{XXX}).

In more recent times, several studies have examined RNNs and methods to train them in greater detail. Graves gives thorough explanations on the different types of sequence labeling tasks that can be modelled using RNNs in the supervised setting both when input and output sequences are aligned or not \hl{XXX} (\hl{XXX}). In his doctoral thesis, Sutskever puts specific emphasis on the problem of properly training RNNs, and provides updates to the model, a second-order optimization algorithm as well as a new initialization scheme \cite{Sutskever:2013:TRN:2604780}. This is particularly interesting for this paper as he recognizes the difficulty of finding a desired solution in an RNN for a given task. 

Deep RNNs, RNNs that have multiple recurrent layers (or cells) stacked on top of each other, were first used to improve the state of the art in speech recognition \cite{graves2013speech}. More specifically, the connectivity across neurons and layers in deep RNNs can be structured in a variety of ways proposed by Pascanu et al. (\citeyear{pascanu2013construct}).

\subsection{Neural networks learning computational tasks}

Since the beginning of artificial neural network research it has been tried to have ANNs learn 

\begin{itemize}
    \item \textit{Separate subsection for theoretical foundations (incl. Turing completeness) of RNNs?}
    \item From a theoretical perspective (which is of particular interest for this paper), Siegelman und Sontag proved that specific instantiations of recurrent neural networks are Turing complete(?) (\citeyear{siegelmann1995computational})
    \item Do universal approximator theorems of FNNs also hold for RNNs? Are RNNs a special case of FNNs?
    \item Here we argue/propose/suggest that the community (should) renew its interest in the computational foundations of artificial neural networks as there exist fundamental issues/shortcomings/constraints that block the advancement of general artificial intelligence
    \item \textit{Cite Grefenstette's summer school talk on RNNs and automata theory}
    \item Grefenstette argues that RNNs when trained approximate rather FSMs(?) than Turing machines and proposes that RNNs be split into controller and memory modules (as many modern approaches implement, cf. NTM etc.). But this leads to the question: how does the biological brain do this split, and does it to it at all? How does the biological brain store memories if not in its connection weights?
    \item
\end{itemize}

\section{Model Definition}

\begin{figure}[!h]
\centering
\begin{tikzpicture}
  % current time step
  \node[output] (yt) {$\vec{y}_t$};
  \node[neuron,below=2em of yt] (htn) {$h_t^n$};
%   \node[neuron,below=of htn] (htm) {$...$};
  \node[neuron,below=3em of htn] (ht1) {$h_t^1$};
  % \node[group,fit={(htn) (htm) (ht1)}] (gr1) {};
  \node[input,below=2em of ht1] (xt) {$\vec{x}_t$};

  \draw[conn] (htn) -- (yt);
%   \draw[conn] (htm) -- (htn);
  \draw[conn] (ht1) -- (htn) node[fill=white, anchor=center, pos=0.5] {...};
  \draw[conn] (xt) -- (ht1);
%   \foreach \destination in {htn,htm,ht1} { % the for loop idea can be expanded to draw the entire diagram quickly
%     \draw[conn] (ht-1n.east) -- (\destination.west);
%   }
  
  % next time step
  \node[neuron,right=of htn] (ht+1n) {$h_{t+1}^n$};
%   \node[neuron,right=of htm] (ht+1m) {$...$};
  \node[neuron,right=of ht1] (ht+11) {$h_{t+1}^1$};
  \node[output,above=2em of ht+1n] (yt+1) {$\vec{y}_{t+1}$};
  \node[input,below=2em of ht+11] (xt+1) {$\vec{x}_{t+1}$};
  
  \draw[conn] (ht+1n) -- (yt+1);
%   \draw[conn] (ht+1m) -- (ht+1n);
  \draw[conn] (ht+11) -- (ht+1n) node[ fill=white, anchor=center, pos=0.5] {...};
  \draw[conn] (xt+1) -- (ht+11);

  \foreach \source in {yt, htn, ht1} {
    \foreach \destination in {yt+1, ht+1n, ht+11} {
      \draw[conn] (\source.east) -- (\destination.west);
    }
  }

  % previous time step
  \node[neuron,left=of htn] (ht-1n) {$h_{t-1}^n$};
%   \node[neuron,left=of htm] (ht-1m) {$...$};
  \node[neuron,left=of ht1] (ht-11) {$h_{t-1}^1$};
  \node[output,above=2em of ht-1n] (yt-1) {$\vec{y}_{t-1}$};
  \node[input,below=2em of ht-11] (xt-1) {$\vec{x}_{t-1}$};
  
  \draw[conn] (ht-1n) -- (yt-1);
%   \draw[conn] (ht-1m) -- (ht-1n);
  \draw[conn] (ht-11) -- (ht-1n) node[fill=white, anchor=center, pos=0.5] {...};
  \draw[conn] (xt-1) -- (ht-11);

  \foreach \source in {yt-1, ht-1n, ht-11} {
    \foreach \destination in {yt, htn, ht1} {
      \draw[conn] (\source.east) -- (\destination.west);
    }
  }
\end{tikzpicture}
\caption{Visualization of ILRNN}
\label{ilrnnviz}
\end{figure}

We consider deep RNNs \cite{graves2013speech}. In the novel architecture called inter-layer recurrent neural networks (ILRNNs), we now extend their connectivity in such a way that both the hidden and output units receive incoming connections from all hidden and output units of the previous time step like so:

\[
h_t^n = \sigma(W_{h^{n-1} h^n} h_t^{n-1} + \sum_i W_{h^i,h^n}^r h_{t-1}^i + b_h^n),
\]

where $h_t^i$ is the hidden activation at time step $t \in \{0, ..., T\}$ and hidden layer $i \in \{1, ..., N\}$, $W$ is the set of feedforward weight matrices, $W^r$ is the set of recurrent weight matrices, $b_h^i$ is the bias vector for hidden layer $i$ and $\sigma$ is an arbitrary activation function.

Figure~\ref{ilrnnviz} shows the connectivity between the different hidden layers across time steps graphically. Note that the output layer is considered the final hidden layer in this notation. Also note that despite top-down connections between layers, since they occur across time steps no cyclical paths on the computational graph are added (allowing this architecture to be trained with conventional BPTT). While this increases the number of free parameters compared to the standard deep RNN considerably, this increase still fares well in comparison with converting a simple RNN cell to an LSTM cell. Surprisingly, to our knowledge this is the first paper to introduce this specific connectivity in recurrent neural networks.

\section{Task Description}

This section describes a specific formulation of the binary addition task that was used to evaluate both the expressibility and learnability of ILRNN.

As outline in 

\begin{table*}[t]
\caption{\hl{Different representations of the addition task}}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Decimal addition & Binary addition & \makecell{Binary addition \\ (reverse order)} & \makecell{Network \\ inputs \& targets} \\
\midrule
\makecell{ \texttt{\ \ 387} \\ \texttt{+\ \ 18} \\ \texttt{= 405} } & \makecell{ \texttt{\ \ 110000011} \\ \texttt{+\ 000010010} \\ \texttt{=\ 110010101} } & \makecell{ \texttt{\ \ 110000011} \\ \texttt{+\ 010010000} \\ \texttt{=\ 101010011} } & 
\begin{tabular}{cccccccccc}
     & $t_1$ & $t_2$ & $t_3$ & $t_4$ & $t_5$ & $t_6$ & $t_7$ & $t_8$ & $t_9$ \\
    $x_1$ &\texttt{1} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} \\
    $x_2$ & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} \\
    $y_1$ & \texttt{1} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} \\
\end{tabular} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{itemize}

    \item \textit{Need my task visualization here :) Table~\ref{sample-table}}
    \item We are evaluating ILRNN on the task to add two binary numbers.
    \item We formulate this task in the following way:
    \item \textit{Finish algorithm or at least logical statements of what is supposed to be computed}
    \item Distinguish our task formulation from the one used for neural program-interpreter \cite{npi}: In their formulation, network also has to learn to memorize the two numbers to be added if I am not mistaken (ah not entirely true, they have an external scratchpad right?). But there are other papers (Graves, NeuralGPU(?))
    \item Task desc: task only required memory of one time step
    \item Also mention parity function/problem (of which XOR is a special case)
    \item See AGI note for all kinds of references (old ones like Rumelhart and recent ones like NPI and NTM)
    \item One advantage of this task is that it naturally provides out-of-sample test instances that can be used to test generalization: simply evaluate the trained model on adding numbers that are larger, i.e. have more digits than the ones seen during training (What exactly does it mean to \textit{generalize})?
    \item The procedure used to generate a sample of a given sequence length is depicted in Algorithm~\ref{alg:binary-addition}. Its assignment statements for $y_1$ will later prove interesting 
\end{itemize}

% [tb] parameter apparently causes algorithm to be top of column
\begin{algorithm}[tb]
   \caption{Generate binary addition sample}
   \label{alg:binary-addition}
\begin{algorithmic}
   \STATE {\bfseries Input:} maximum digits per addend $n$, 
   % \REPEAT
   \STATE Initialize $carry = false$, $sample = []$
   % $x_1 = $, $x_2 = $.
   \FOR{$i=1$ {\bfseries to} $n$}
   \STATE Draw $x_1$, $x_2$ uniformly from $\{0, 1\}$
%   \IF{$carry == false$}
   \STATE $y_1 = (x_1 \oplus x_2) \oplus carry$
   \STATE $carry = x_1 \land x_2 + carry \land (x_1 \oplus x_2)$
%   \ELSE
%   \STATE \hl{$y_1 = XXX$, $carry = XXX$}
%   \ENDIF
   \STATE Append $(x_1, x_2, y_1)$ to $sample$
   \ENDFOR
   \IF{$carry == true$}
   \STATE Append $(0,0,1)$ to $sample$
   \ENDIF
   \STATE Return $sample$
   % Should this algorithm be a generator? But isn't generator something
   % Python specific? So many we don't even use the REPEAT here?
   % \UNTIL{$noChange$ is $true$}
   % \RETURN $sample$
\end{algorithmic}
\end{algorithm}

\section{Experiments}

After introducing a new recurrent neural network model and specifying a specific form of binary addition task, the results of the experiments are reported. It is important to note that no experiment reported in this paper was conducted without incorporating prior knowledge into the model architecture. Accordingly, first the manual construction or rather deduction of the hyperparameter settings will be explained, before an attempt for an analytic solution and training using an gradient descent algorithm.

\subsection{Manual construction}

% \begin{figure*}[t]
% \begin{center}

% \begin{subfigure}[t]
% \begin{circuitikz} \draw
% (0,2) node[and port] (myand1) {}
% (0,0) node[and port] (myand2) {}
% (2,1) node[xnor port] (myxnor) {}
% (myand1.out) -- (myxnor.in 1)
% (myand2.out) -- (myxnor.in 2);
% \end{circuitikz}
% \end{subfigure}

% % \begin{subfigure}[t]
% % \begin{circuitikz} \draw
% % (0,2) node[and port] (myand1) {}
% % (0,0) node[and port] (myand2) {}
% % (2,1) node[xnor port] (myxnor) {}
% % (myand1.out) -- (myxnor.in 1)
% % (myand2.out) -- (myxnor.in 2);
% % \end{circuitikz}
% % \end{subfigure}

% \end{center}
% \end{figure*}

% \begin{figure*}[t]
% % \centering
% \begin{subfigure}[t]{0.3\pagewidth}
% % \centering
% % \begin{circuitikz}[ scale=1.2, american voltages]\draw
% %  (0,0) -- (3,0) to[short, -o](3,0)
% %  (0,1.5) to [R, l=$10$] (3,1.5) to[short, -o](3,1.5)
% %  (3,1.5) to [open, v = $V_c$] (3,0)
% %  (0,0) to [V, l=$V_i$] (0,1.5)
% %  ;\end{circuitikz}
% \begin{circuitikz} 
% \draw
% (0,2) node[and port] (myand1) {}
% (0,0) node[and port] (myand2) {}
% (2,1) node[xnor port] (myxnor) {}
% (myand1.out) -- (myxnor.in 1)
% (myand2.out) -- (myxnor.in 2);
% \end{circuitikz}
% % \caption{Steady State Capacitor}
% % \label{Steady State Capacitor}
% \end{subfigure}

% \hspace{1.1cm}

% % \centering
% \begin{subfigure}[t]{0.5\pagewidth}
% % \centering
% % \begin{circuitikz}[scale=1.2, american voltages]
% % \draw
% % (0,0) -- (3,0) to[short, -o](3,0)
% % (0,1.5) to [R, l=$10$] (3,1.5) to[short, -o](3,1.5)
% % (3,1.5) to [open, v = $V_c$] (3,0)
% % (0,0) to [V, l=$V_i$] (0,1.5);
% % \end{circuitikz}

% \begin{circuitikz} 
% \draw
% (0,2) node[and port] (myand1) {}
% (0,0) node[and port] (myand2) {}
% (2,1) node[xnor port] (myxnor) {}
% (myand1.out) -- (myxnor.in 1)
% (myand2.out) -- (myxnor.in 2);
% \end{circuitikz}

% % \caption{Steady State Capacitor}
% \end{subfigure}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering

        % \begin{circuitikz}
        %   \draw
        %   (3,0) node[xor port] (myxor) {} to
        %   (7,0) node[xor port,anchor=in 2] (myxor1) {}
        %   (0,-3) node[and port,rotate=270] (myand) {}
        %   (5,-3) node[and port,rotate=270] (myand1) {}
          
        %   (2.5,-6) node[or port,rotate=270] (myor) {}
        %   (myxor.in 1) -- +(-2.5,0) node[anchor=east] (a) {A}
        %   (myxor.in 2) -- +(-2.5,0) node[anchor=east] (b) {B}
        %   (myor.out) node[anchor=north] (co) {Carry out}
        %   (myxor.in 2 -| myand.in 1) node[circ] {} -- (myand.in 1)
        %   (myxor.in 1 -| myand.in 2) node[circ] {} -- (myand.in 2)
        %   (myand.out) |- (myor.in 2)
        %   (myand1.out) |- (myor.in 1)
          
        %   (myand1.in 1) -- +(0,2.75) node[anchor=south] (cin) {Carry in}
        %   (myand1.in 1 |- myxor1.in 1) node[circ] {} -- (myxor1.in 1)
        %   (myxor1.in 2 -| myand1.in 2) node[circ] {} -- (myand1.in 2)
        %   (myxor1.out) node[anchor=west] (sum) {Sum}
        %   ;
        % \end{circuitikz}
        \includegraphics[width=\textwidth]{full-adder-circuit}

        \caption{Simple full adder circuit}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
    % \begin{subfigure}
        % \includegraphics[width=\textwidth]{full-adder-ilrnn}
    % \end{subfigure}
    % \begin{tikzpicture}%[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    %     \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    %     \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    %     \tikzstyle{input neuron}=[neuron, fill=green!50];
    %     \tikzstyle{output neuron}=[neuron, fill=red!50];
    %     \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    %     \tikzstyle{annot} = [text width=4em, text centered]
    
    %     % Draw the input layer nodes
    %     \foreach \name / \y in {1,...,4}
    %     % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
    %         \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};
    
    %     % Draw the hidden layer nodes
    %     \foreach \name / \y in {1,...,5}
    %         \path[yshift=0.5cm]
    %             node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
    
    %     % Draw the output layer node
    %     \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};
    
    %     % Connect every node in the input layer with every node in the
    %     % hidden layer.
    %     \foreach \source in {1,...,4}
    %         \foreach \dest in {1,...,5}
    %             \path (I-\source) edge (H-\dest);
    
    %     % Connect every node in the hidden layer with the output layer
    %     \foreach \source in {1,...,5}
    %         \path (H-\source) edge (O);
    
    %     % Annotate the layers
    %     \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
    %     \node[annot,left of=hl] {Input layer};
    %     \node[annot,right of=hl] {Output layer};
    % \end{tikzpicture}
    % \end{subfigure}

    %     \begin{circuitikz} 
    %     \draw
    %     (0,2) node[and port] (myand1) {}
    %     (0,0) node[and port] (myand2) {}
    %     (2,1) node[xnor port] (myxnor) {}
    %     (myand1.out) -- (myxnor.in 1)
    %     (myand2.out) -- (myxnor.in 2);
    %     \end{circuitikz}
        \includegraphics[width=\textwidth]{full-adder-ilrnn}
        \caption{Simple full adder ILRNN}
    %     \caption{Lorem ipsum, lorem ipsum,Lorem ipsum, lorem ipsum,Lorem ipsum}
    \end{subfigure}
    \caption{\hl{Full adder circuit and ILRNN equivalent}}
    \label{fulladder}
\end{figure*}


\begin{itemize}
    \item \textit{Full adder microcircuit can serve as the reasoning behind my choice of hyperparameters / architecture \ref{fulladder}}
    \item \textit{Jup, both preceding points are valid. Pretty much resembles my "solution" topic from earlier structure draft}
    \item \textit{My microcircuit + neural equivalent visualization is coming here. Note that I do have to explain how I made this analogy (using basic logic gates).} 
    \item \textit{Cite Jordan(?) for XOR network instantiation}
    \item \textit{Cite "Learning XOR - exploring the space of a classic problem"}
    \item Note that this is more elegant than the full adder circuit because it needs one full adder circuit for every digit in the addends (no recurrence afaik?)
    \item Note that the full adder (and more broadly, arithmetic logic units) are instantiations of \textit{combination logic} which according to Chomsky (XXX cite Chlomsky) are a class of automata less expressive than even finite state machines
\end{itemize}

\subsection{Analytical solution}
\begin{itemize}
    \item Firstly, we address the question: Does there exist a recurrent neural network that is able to perfectly solve / represent a perfect solution to the binary addition task? This we tried using analytical solutions first. +linear activation 
    \item \textit{Do I mention my approach using Gr\"{o}bner bases here? Yes, also add some maths for it}
    \item Since conventional learning algorithms are iterative / convex optimization, it is reasonable to ask whether the toy problem posed here can be solved analytically (\textit{This sentence doesn't make sense in itself yet because how do I jump from optimization to solving?}
    \item \textit{Did I even do this on a proper architecture? Did I try this on the full adder architecture with linear activations?}
    \item \textit{\hl{Can I say that there exists no solution to the simple deep RNN w/o inter-layer recurrent connections?}}
\end{itemize}

\subsection{Learning / iterative optimization}
\begin{itemize}
    \item Finally, we regard the task as a supervised sequence learning problem and train the network using backpropagation through time (BPTT) \cite{rumelhart1986learning}.
    \item Custom activation function used to resemble microcircuit (this I can define using math as well). Cite XOR-Net \cite{RastegariORF16} and binarized neural networks \cite{DBLP:journals/corr/CourbariauxB16}.
    \item Show graph of accuracy over sequence length (how it slowly degrades for normal RNN) vs. how it stays at 100\% for manual solution / ILRNN (depending on what I can achieve here...)
    \item That paper that examines error surface of XOR
\end{itemize}

\subsection{XXX}

XXX.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\section{Conclusion \& Future Work}

XXX.

\begin{itemize}
    \item This binary addition task seems like a good test bed for further research about exact learning in neural networks
    \item So far, architecture was hand-crafted (and weights needed to be initialized close to hand-crafted solution?), which is far from practical
    \item We have illustrated the potential of inter-layer recurrent connections to facilitate algorithm learning in neural networks.
    \item Have the proposed network as a subnetwork of a larger network
    \item Try other (biologically inspired) learning algorithms, such as 
    \item So far we have ausgelagert the problem of memory by feeding digits two at a time. Incorporate notions of memory (both internal and external to the neural core)
    \item Still needs to be evaluated how well the new architecture fares with larger problems and in combination with other recent deep learning-related refinements such as LSTM cells, regularization, bi-directionality etc.
    \item Regarding larger problems, consider also those not requiring exact learning (potentially cite \textit{exact learning} somewhere?)
    \item Now that per-layer constraints in connectivity have been loosened, it would also be interesting to ask whether the law of synchrony of computation of neurons in artificial neural networks can be loosened
    \item The learning algorithm has no "intention"/"inclination" yet to learn the general solution
    \item Asynchronously firing neurons (ah well, currently its firing \textit{rates}, right?)
\end{itemize}

% We strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, do not
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the CMT reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{ilrnn_icml19}
\bibliographystyle{icml2019}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
